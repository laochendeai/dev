#!/usr/bin/env python3
"""
æ¨¡å‹è¯„ä¼°å’ŒéªŒè¯ç³»ç»Ÿ
ä¸“ä¸šçš„æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½è¯„ä¼°å·¥å…·
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.model_selection import cross_val_score, TimeSeriesSplit
import logging
import json
from datetime import datetime

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self):
        self.evaluation_results = {}
        self.config = {
            'cv_splits': 5,
            'test_size': 0.2,
            'random_state': 42
        }

    def basic_metrics(self, y_true, y_pred, y_prob=None):
        """åŸºç¡€åˆ†ç±»æŒ‡æ ‡"""
        metrics = {}

        # åŸºç¡€æŒ‡æ ‡
        metrics['accuracy'] = accuracy_score(y_true, y_pred)
        metrics['precision'] = precision_score(y_true, y_pred, average='weighted')
        metrics['recall'] = recall_score(y_true, y_pred, average='weighted')
        metrics['f1_score'] = f1_score(y_true, y_pred, average='weighted')

        # å¦‚æœæœ‰æ¦‚ç‡é¢„æµ‹ï¼Œè®¡ç®—AUC
        if y_prob is not None:
            try:
                metrics['auc_roc'] = roc_auc_score(y_true, y_prob[:, 1])
            except:
                metrics['auc_roc'] = None

        # æ··æ·†çŸ©é˜µ
        metrics['confusion_matrix'] = confusion_matrix(y_true, y_pred).tolist()

        return metrics

    def time_series_split_validation(self, model, X, y):
        """æ—¶é—´åºåˆ—åˆ†å‰²éªŒè¯"""
        logger.info("â° å¼€å§‹æ—¶é—´åºåˆ—åˆ†å‰²éªŒè¯")

        tscv = TimeSeriesSplit(n_splits=self.config['cv_splits'])
        cv_scores = []

        for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):
            X_train, X_test = X[train_idx], X[test_idx]
            y_train, y_test = y[train_idx], y[test_idx]

            model.fit(X_train, y_train)
            score = model.score(X_test, y_test)
            cv_scores.append(score)

            logger.info(f"   Fold {fold + 1}: {score:.4f}")

        cv_results = {
            'cv_scores': cv_scores,
            'mean_score': np.mean(cv_scores),
            'std_score': np.std(cv_scores),
            'cv_folds': self.config['cv_splits']
        }

        logger.info(f"   å¹³å‡åˆ†æ•°: {cv_results['mean_score']:.4f} (Â±{cv_results['std_score']:.4f})")

        return cv_results

    def financial_metrics(self, y_true, y_pred, returns):
        """é‡‘èç›¸å…³æŒ‡æ ‡"""
        financial_metrics = {}

        # åªåœ¨é¢„æµ‹æ­£ç¡®çš„æ ·æœ¬ä¸Šè®¡ç®—æ”¶ç›Š
        correct_predictions = y_true == y_pred
        correct_returns = returns[correct_predictions]

        if len(correct_returns) > 0:
            financial_metrics['avg_return_correct'] = np.mean(correct_returns)
            financial_metrics['total_return_correct'] = np.prod(1 + correct_returns) - 1

        # æ•´ä½“æ”¶ç›Š
        if len(returns) > 0:
            financial_metrics['avg_return_all'] = np.mean(returns)
            financial_metrics['total_return_all'] = np.prod(1 + returns) - 1

        # å¤æ™®æ¯”ç‡ï¼ˆå¹´åŒ–ï¼‰
        if len(returns) > 1:
            financial_metrics['sharpe_ratio'] = np.mean(returns) / np.std(returns) * np.sqrt(252 * 24 * 12)  # å‡è®¾5åˆ†é’Ÿæ•°æ®

        # æœ€å¤§å›æ’¤
        if len(returns) > 0:
            cumulative = np.cumprod(1 + returns)
            peak = np.maximum.accumulate(cumulative)
            drawdown = (cumulative - peak) / peak
            financial_metrics['max_drawdown'] = np.min(drawdown)

        # èƒœç‡
        financial_metrics['win_rate'] = np.mean(y_true == y_pred)

        return financial_metrics

    def detailed_classification_report(self, y_true, y_pred, target_names=['ä¸‹è·Œ/æ¨ªç›˜', 'ä¸Šæ¶¨']):
        """è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š"""
        report = classification_report(y_true, y_pred, target_names=target_names, output_dict=True)
        return report

    def feature_importance_analysis(self, model, feature_names):
        """ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        if hasattr(model, 'feature_importances_'):
            importance = model.feature_importances_
            feature_importance = dict(zip(feature_names, importance))

            # æŒ‰é‡è¦æ€§æ’åº
            sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)

            return sorted_importance
        else:
            return None

    def plot_confusion_matrix(self, y_true, y_pred, labels=['ä¸‹è·Œ/æ¨ªç›˜', 'ä¸Šæ¶¨']):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        plt.figure(figsize=(8, 6))
        cm = confusion_matrix(y_true, y_pred)

        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=labels, yticklabels=labels)
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.tight_layout()
        plt.show()

    def plot_feature_importance(self, feature_importance, top_n=15):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§"""
        if feature_importance:
            plt.figure(figsize=(10, 8))

            # å–å‰Nä¸ªé‡è¦ç‰¹å¾
            top_features = feature_importance[:top_n]
            features, importances = zip(*top_features)

            plt.barh(range(len(features)), importances)
            plt.yticks(range(len(features)), features)
            plt.xlabel('ç‰¹å¾é‡è¦æ€§')
            plt.title(f'Top {top_n} ç‰¹å¾é‡è¦æ€§')
            plt.gca().invert_yaxis()
            plt.tight_layout()
            plt.show()

    def comprehensive_evaluation(self, model, X, y, feature_names=None, returns=None):
        """ç»¼åˆè¯„ä¼°"""
        logger.info("ğŸ” å¼€å§‹ç»¼åˆæ¨¡å‹è¯„ä¼°")

        # åˆ†å‰²æ•°æ®
        split_idx = int(len(X) * (1 - self.config['test_size']))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)

        # é¢„æµ‹
        y_pred = model.predict(X_test)
        if hasattr(model, 'predict_proba'):
            y_prob = model.predict_proba(X_test)
        else:
            y_prob = None

        # åŸºç¡€æŒ‡æ ‡
        basic_metrics = self.basic_metrics(y_test, y_pred, y_prob)

        # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        cv_results = self.time_series_split_validation(model, X, y)

        # é‡‘èæŒ‡æ ‡ï¼ˆå¦‚æœæœ‰æ”¶ç›Šæ•°æ®ï¼‰
        if returns is not None and len(returns) == len(y):
            test_returns = returns[split_idx:]
            financial_metrics = self.financial_metrics(y_test, y_pred, test_returns)
        else:
            financial_metrics = {}

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        detailed_report = self.detailed_classification_report(y_test, y_pred)

        # ç‰¹å¾é‡è¦æ€§
        if feature_names is not None:
            feature_importance = self.feature_importance_analysis(model, feature_names)
        else:
            feature_importance = None

        # æ±‡æ€»ç»“æœ
        evaluation_results = {
            'timestamp': datetime.now().isoformat(),
            'test_size': len(y_test),
            'basic_metrics': basic_metrics,
            'cross_validation': cv_results,
            'financial_metrics': financial_metrics,
            'detailed_report': detailed_report,
            'feature_importance': feature_importance
        }

        # æ‰“å°ç»“æœ
        self.print_evaluation_results(evaluation_results)

        return evaluation_results

    def print_evaluation_results(self, results):
        """æ‰“å°è¯„ä¼°ç»“æœ"""
        print("\n" + "="*60)
        print("ğŸ“Š æ¨¡å‹è¯„ä¼°ç»“æœ")
        print("="*60)

        # åŸºç¡€æŒ‡æ ‡
        basic = results['basic_metrics']
        print(f"\nğŸ“ˆ åŸºç¡€æŒ‡æ ‡:")
        print(f"   å‡†ç¡®ç‡ (Accuracy): {basic['accuracy']:.4f}")
        print(f"   ç²¾ç¡®ç‡ (Precision): {basic['precision']:.4f}")
        print(f"   å¬å›ç‡ (Recall): {basic['recall']:.4f}")
        print(f"   F1åˆ†æ•°: {basic['f1_score']:.4f}")
        if basic.get('auc_roc'):
            print(f"   AUC-ROC: {basic['auc_roc']:.4f}")

        # äº¤å‰éªŒè¯
        cv = results['cross_validation']
        print(f"\nâ° æ—¶é—´åºåˆ—äº¤å‰éªŒè¯:")
        print(f"   å¹³å‡åˆ†æ•°: {cv['mean_score']:.4f} Â± {cv['std_score']:.4f}")
        print(f"   CVåˆ†æ•°: {[f'{score:.4f}' for score in cv['cv_scores']]}")

        # é‡‘èæŒ‡æ ‡
        if results['financial_metrics']:
            fin = results['financial_metrics']
            print(f"\nğŸ’° é‡‘èæŒ‡æ ‡:")
            print(f"   èƒœç‡: {fin['win_rate']:.4f}")
            if fin.get('avg_return_correct'):
                print(f"   æ­£ç¡®é¢„æµ‹å¹³å‡æ”¶ç›Š: {fin['avg_return_correct']:.4f}")
            if fin.get('sharpe_ratio'):
                print(f"   å¤æ™®æ¯”ç‡: {fin['sharpe_ratio']:.4f}")
            if fin.get('max_drawdown'):
                print(f"   æœ€å¤§å›æ’¤: {fin['max_drawdown']:.4f}")

        # ç‰¹å¾é‡è¦æ€§
        if results['feature_importance']:
            print(f"\nğŸ” Top 10 é‡è¦ç‰¹å¾:")
            for i, (feature, importance) in enumerate(results['feature_importance'][:10], 1):
                print(f"   {i:2d}. {feature:<20} {importance:.4f}")

        print("\n" + "="*60)

    def save_evaluation_results(self, results, filename='model_evaluation.json'):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° {filename}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            return False

def demo_evaluation():
    """æ¼”ç¤ºæ¨¡å‹è¯„ä¼°"""
    print("ğŸ” æ¨¡å‹è¯„ä¼°ç³»ç»Ÿæ¼”ç¤º")
    print("="*50)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    print("\nğŸ“Š åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®...")
    np.random.seed(42)
    n_samples = 1000
    n_features = 10

    # æ¨¡æ‹Ÿç‰¹å¾
    X = np.random.randn(n_samples, n_features)

    # æ¨¡æ‹Ÿæ ‡ç­¾ï¼ˆæœ‰ä¸€å®šé€»è¾‘æ€§ï¼‰
    y = (X[:, 0] + 0.5 * X[:, 1] + np.random.randn(n_samples) * 0.5 > 0).astype(int)

    # æ¨¡æ‹Ÿæ”¶ç›Šç‡
    returns = np.random.randn(n_samples) * 0.001

    # ç‰¹å¾åç§°
    feature_names = [f'feature_{i+1}' for i in range(n_features)]

    print(f"âœ… æ•°æ®åˆ›å»ºå®Œæˆ: {n_samples} æ ·æœ¬, {n_features} ç‰¹å¾")
    print(f"   æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(y)}")
    print(f"   æ”¶ç›Šç‡èŒƒå›´: [{returns.min():.4f}, {returns.max():.4f}]")

    # åˆ›å»ºè¯„ä¼°å™¨
    from sklearn.ensemble import RandomForestClassifier
    evaluator = ModelEvaluator()

    # åˆ›å»ºæ¨¡å‹
    model = RandomForestClassifier(n_estimators=100, random_state=42)

    # è¿è¡Œè¯„ä¼°
    print("\nğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    results = evaluator.comprehensive_evaluation(model, X, y, feature_names, returns)

    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    print("\nğŸ“Š ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
    evaluator.plot_confusion_matrix(
        results['basic_metrics']['confusion_matrix'],
        labels=['ä¸‹è·Œ/æ¨ªç›˜', 'ä¸Šæ¶¨']
    )

    # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§
    print("\nğŸ“Š ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§...")
    evaluator.plot_feature_importance(results['feature_importance'])

    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    evaluator.save_evaluation_results(results)

    print("\nğŸ‰ æ¨¡å‹è¯„ä¼°æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    demo_evaluation()