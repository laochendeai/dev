#!/usr/bin/env python3
"""
æ•°æ®ç®¡ç†å’Œå‡†å¤‡ç³»ç»Ÿ
ä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹æä¾›é«˜è´¨é‡çš„æ•°æ®
"""

import ccxt
import pandas as pd
import numpy as np
import time
import sqlite3
import logging
from datetime import datetime, timedelta
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, db_path="market_data.db"):
        self.db_path = db_path
        self.exchange = ccxt.binance()
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # åˆ›å»ºKçº¿æ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS klines_5m (
                    timestamp INTEGER PRIMARY KEY,
                    open REAL,
                    high REAL,
                    low REAL,
                    close REAL,
                    volume REAL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # åˆ›å»ºç‰¹å¾æ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS features (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp INTEGER,
                    returns REAL,
                    rsi REAL,
                    ma5 REAL,
                    ma10 REAL,
                    ma20 REAL,
                    bb_position REAL,
                    volatility REAL,
                    volume_ratio REAL,
                    momentum REAL,
                    label INTEGER,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            conn.commit()
            conn.close()
            logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")

    def fetch_historical_data(self, symbol='ETH/USDT', timeframe='5m', limit=1000):
        """è·å–å†å²æ•°æ®"""
        try:
            logger.info(f"ğŸ“¥ è·å– {symbol} {timeframe} å†å²æ•°æ®ï¼Œæ•°é‡: {limit}")

            ohlcv = self.exchange.fetch_ohlcv(symbol, timeframe, limit=limit)

            df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')

            logger.info(f"âœ… è·å–åˆ° {len(df)} æ¡æ•°æ®")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {df['timestamp'].min()} åˆ° {df['timestamp'].max()}")

            return df

        except Exception as e:
            logger.error(f"âŒ è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            return None

    def save_to_database(self, df):
        """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)

            # è½¬æ¢æ—¶é—´æˆ³ä¸ºUnixæ—¶é—´æˆ³
            df_to_save = df.copy()
            df_to_save['timestamp'] = df_to_save['timestamp'].astype(int) // 1000

            # ä¿å­˜Kçº¿æ•°æ®
            df_to_save[['timestamp', 'open', 'high', 'low', 'close', 'volume']].to_sql(
                'klines_5m', conn, if_exists='replace', index=False
            )

            conn.commit()
            conn.close()

            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(df)} æ¡æ•°æ®åˆ°æ•°æ®åº“")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False

    def calculate_features(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        logger.info("ğŸ”§ è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")

        df = df.copy()

        # åŸºç¡€ä»·æ ¼æŒ‡æ ‡
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))

        # ç§»åŠ¨å¹³å‡çº¿
        df['ma5'] = df['close'].rolling(5).mean()
        df['ma10'] = df['close'].rolling(10).mean()
        df['ma20'] = df['close'].rolling(20).mean()

        # RSI
        delta = df['close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # å¸ƒæ—å¸¦
        df['bb_middle'] = df['close'].rolling(20).mean()
        bb_std = df['close'].rolling(20).std()
        df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
        df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
        df['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])

        # æ³¢åŠ¨ç‡
        df['volatility'] = df['returns'].rolling(10).std()

        # æˆäº¤é‡æŒ‡æ ‡
        df['volume_ma'] = df['volume'].rolling(10).mean()
        df['volume_ratio'] = df['volume'] / df['volume_ma']

        # åŠ¨é‡
        df['momentum'] = df['close'].pct_change(5)

        # ä»·æ ¼ç›¸å¯¹ä½ç½®
        df['price_position'] = (df['close'] - df['low'].rolling(20).min()) / \
                              (df['high'].rolling(20).max() - df['low'].rolling(20).min())

        logger.info(f"âœ… è®¡ç®—äº† {len(df.columns)} ä¸ªç‰¹å¾")
        return df

    def create_labels(self, df, horizon=1, threshold=0.001):
        """åˆ›å»ºé¢„æµ‹æ ‡ç­¾"""
        logger.info(f"ğŸ·ï¸ åˆ›å»ºé¢„æµ‹æ ‡ç­¾ (horizon={horizon}, threshold={threshold})")

        df = df.copy()

        # è®¡ç®—æœªæ¥æ”¶ç›Š
        df['future_return'] = df['close'].shift(-horizon) / df['close'] - 1

        # åˆ›å»ºæ ‡ç­¾
        df['label'] = (df['future_return'] > threshold).astype(int)

        # ç§»é™¤æ— æ³•è®¡ç®—æ ‡ç­¾çš„æœ€åå‡ è¡Œ
        df = df.dropna(subset=['future_return', 'label'])

        label_dist = df['label'].value_counts()
        logger.info(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: ä¸Šæ¶¨={label_dist.get(1, 0)}, ä¸‹è·Œ/æ¨ªç›˜={label_dist.get(0, 0)}")

        return df

    def prepare_ml_dataset(self, symbol='ETH/USDT', limit=1000):
        """å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®é›†"""
        logger.info("ğŸš€ å¼€å§‹å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®é›†")

        # 1. è·å–åŸå§‹æ•°æ®
        raw_data = self.fetch_historical_data(symbol, '5m', limit)
        if raw_data is None:
            return None

        # 2. è®¡ç®—ç‰¹å¾
        feature_data = self.calculate_features(raw_data)

        # 3. åˆ›å»ºæ ‡ç­¾
        labeled_data = self.create_labels(feature_data)

        # 4. é€‰æ‹©æœ€ç»ˆçš„ç‰¹å¾åˆ—
        feature_columns = [
            'returns', 'log_returns', 'rsi', 'ma5', 'ma10', 'ma20',
            'bb_position', 'volatility', 'volume_ratio', 'momentum', 'price_position'
        ]

        # 5. æ¸…ç†æ•°æ®
        final_data = labeled_data[feature_columns + ['label']].dropna()

        if len(final_data) < 100:
            logger.error(f"âŒ æ•°æ®ä¸è¶³: {len(final_data)} < 100")
            return None

        logger.info(f"âœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ: {len(final_data)} æ ·æœ¬, {len(feature_columns)} ç‰¹å¾")

        return final_data

    def save_features_to_db(self, df):
        """ä¿å­˜ç‰¹å¾æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            conn = sqlite3.connect(self.db_path)

            # å‡†å¤‡æ•°æ®
            feature_df = df.copy()
            feature_df['timestamp'] = feature_df.index.astype(int) // 1000

            # é€‰æ‹©è¦ä¿å­˜çš„åˆ—
            save_columns = ['timestamp', 'returns', 'rsi', 'ma5', 'ma10', 'ma20',
                           'bb_position', 'volatility', 'volume_ratio', 'momentum', 'label']

            feature_df[save_columns].to_sql('features', conn, if_exists='replace', index=False)

            conn.commit()
            conn.close()

            logger.info(f"ğŸ’¾ æˆåŠŸä¿å­˜ {len(feature_df)} æ¡ç‰¹å¾æ•°æ®")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç‰¹å¾æ•°æ®å¤±è´¥: {e}")
            return False

    def get_latest_data(self, count=50):
        """è·å–æœ€æ–°çš„æ•°æ®"""
        try:
            conn = sqlite3.connect(self.db_path)

            query = f'''
                SELECT * FROM klines_5m
                ORDER BY timestamp DESC
                LIMIT {count}
            '''

            df = pd.read_sql_query(query, conn)
            conn.close()

            if not df.empty:
                df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                df = df.sort_values('timestamp').reset_index(drop=True)

            return df

        except Exception as e:
            logger.error(f"âŒ è·å–æœ€æ–°æ•°æ®å¤±è´¥: {e}")
            return None

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ•°æ®ç®¡ç†æµç¨‹"""
    print("ğŸ“Š æ•°æ®ç®¡ç†ç³»ç»Ÿæ¼”ç¤º")
    print("="*50)

    # åˆ›å»ºæ•°æ®ç®¡ç†å™¨
    dm = DataManager()

    # å‡†å¤‡æ•°æ®é›†
    print("\nğŸš€ æ­¥éª¤1: å‡†å¤‡æœºå™¨å­¦ä¹ æ•°æ®é›†")
    dataset = dm.prepare_ml_dataset(limit=500)

    if dataset is not None:
        print(f"âœ… æ•°æ®é›†å‡†å¤‡æˆåŠŸ!")
        print(f"   æ ·æœ¬æ•°é‡: {len(dataset)}")
        print(f"   ç‰¹å¾æ•°é‡: {len(dataset.columns) - 1}")
        print(f"   æ ‡ç­¾åˆ†å¸ƒ: {dataset['label'].value_counts().to_dict()}")

        # ä¿å­˜åˆ°æ•°æ®åº“
        print("\nğŸ’¾ æ­¥éª¤2: ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“")
        success = dm.save_features_to_db(dataset)

        if success:
            print("âœ… æ•°æ®ä¿å­˜æˆåŠŸ!")
        else:
            print("âŒ æ•°æ®ä¿å­˜å¤±è´¥!")
    else:
        print("âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥!")

    # æµ‹è¯•æ•°æ®è·å–
    print("\nğŸ“¥ æ­¥éª¤3: æµ‹è¯•æ•°æ®è·å–")
    latest_data = dm.get_latest_data(20)

    if latest_data is not None:
        print(f"âœ… è·å–æœ€æ–°æ•°æ®æˆåŠŸ: {len(latest_data)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {latest_data['timestamp'].min()} åˆ° {latest_data['timestamp'].max()}")
        print(f"   æœ€æ–°ä»·æ ¼: ${latest_data['close'].iloc[-1]:.2f}")
    else:
        print("âŒ è·å–æœ€æ–°æ•°æ®å¤±è´¥!")

    print("\nğŸ‰ æ•°æ®ç®¡ç†æ¼”ç¤ºå®Œæˆ!")

if __name__ == "__main__":
    main()